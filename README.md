# MultiFrame-LPR

Multi-frame OCR solution for the **ICPR 2026 Challenge on Low-Resolution License Plate Recognition**.

This implementation combines temporal information from 5 video frames using attention fusion mechanisms to achieve robust recognition on low-resolution license plates.

ðŸ”— **Challenge:** [ICPR 2026 LRLPR](https://icpr26lrlpr.github.io/)

---

## Quick Start

```bash
# Install dependencies
uv sync

# Train with default settings (ResTranOCR + STN)
python train.py

# Train CRNN baseline
python train.py --model crnn --experiment-name crnn_baseline

# Generate submission file
python train.py --submission-mode --model restran
```

---

## Key Features

- **Multi-Frame Fusion**: Processes 5-frame sequences with attention-based fusion
- **Spatial Transformer Network**: Optional STN module for automatic image alignment
- **Dual Architectures**: CRNN (baseline) and ResTranOCR (ResNet34 + Transformer)
- **Smart Data Augmentation**: Scenario-B aware validation split with configurable augmentation levels
- **Production Ready**: Mixed precision training, gradient clipping, OneCycleLR scheduler

---

## Model Architectures

### CRNN (Baseline)
**Pipeline:** Multi-frame Input â†’ STN Alignment â†’ CNN â†’ Attention Fusion â†’ BiLSTM â†’ CTC

Simple and effective baseline using convolutional features and bidirectional LSTM for sequence modeling.

### ResTranOCR (Advanced)
**Pipeline:** Multi-frame Input â†’ STN Alignment â†’ ResNet34 â†’ Attention Fusion â†’ Transformer â†’ CTC

Modern architecture leveraging ResNet34 backbone and Transformer encoder with positional encoding for improved long-range dependencies.

**Both models accept input shape:** `(Batch, 5, 3, 32, 128)` and output character sequences via CTC decoding.

---

## Installation

**Requirements:**
- Python 3.11+
- CUDA-enabled GPU (recommended)

**Using uv (recommended):**
```bash
git clone https://github.com/duongtruongbinh/MultiFrame-LPR.git
cd MultiFrame-LPR
uv sync
```

**Using pip:**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip install albumentations opencv-python matplotlib numpy pandas tqdm
```

---

## Usage

### Data Preparation

Organize your dataset with the following structure:

```
data/train/
â”œâ”€â”€ track_001/
â”‚   â”œâ”€â”€ lr-001.png
â”‚   â”œâ”€â”€ lr-002.png
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ hr-001.png (optional, for synthetic LR generation)
â”‚   â””â”€â”€ annotations.json
â””â”€â”€ track_002/
    â””â”€â”€ ...
```

**annotations.json format:**
```json
{"plate_text": "ABC1234"}
```

### Training

**Basic training:**
```bash
python train.py
```

**Custom configuration:**
```bash
python train.py \
    --model restran \
    --experiment-name my_experiment \
    --data-root /path/to/dataset \
    --batch-size 64 \
    --epochs 30 \
    --lr 0.0005 \
    --aug-level full
```

**Disable STN:**
```bash
python train.py --no-stn
```

**Key arguments:**
- `-m, --model`: Model type (`crnn` or `restran`)
- `-n, --experiment-name`: Experiment identifier
- `--data-root`: Path to training data (default: `data/train`)
- `--batch-size`: Batch size (default: 64)
- `--epochs`: Training epochs (default: 30)
- `--lr`: Learning rate (default: 5e-4)
- `--aug-level`: Augmentation level (`full` or `light`)
- `--no-stn`: Disable Spatial Transformer Network
- `--submission-mode`: Train on full dataset and generate test predictions
- `--output-dir`: Output directory (default: `results/`)

### Ablation Studies

Run automated experiments comparing different configurations:

```bash
python run_ablation.py
```

Experiments:
- CRNN with/without STN
- ResTranOCR with/without STN

Results saved in `experiments/ablation_summary.txt`.

### Outputs

After training, the following files are generated in the output directory:

- `{experiment_name}_best.pth` - Best model checkpoint
- `submission_{experiment_name}.txt` - Predictions in competition format: `track_id,predicted_text;confidence`

---

## Configuration

Key hyperparameters in `configs/config.py`:

```python
MODEL_TYPE = "restran"           # "crnn" or "restran"
USE_STN = True                   # Enable/disable STN
BATCH_SIZE = 64
LEARNING_RATE = 5e-4
EPOCHS = 30
AUGMENTATION_LEVEL = "full"      # "full" or "light"

# CRNN specific
HIDDEN_SIZE = 256
RNN_DROPOUT = 0.25

# ResTranOCR specific
TRANSFORMER_HEADS = 8
TRANSFORMER_LAYERS = 3
TRANSFORMER_FF_DIM = 2048
TRANSFORMER_DROPOUT = 0.1
```

All config parameters can be overridden via CLI arguments.

---

## Project Structure

```
.
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.py              # Configuration dataclass
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py         # MultiFrameDataset with scenario-aware splitting
â”‚   â”‚   â””â”€â”€ transforms.py      # Augmentation pipelines
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ crnn.py            # CRNN baseline
â”‚   â”‚   â”œâ”€â”€ restran.py         # ResTranOCR advanced model
â”‚   â”‚   â””â”€â”€ components.py      # Shared modules (STN, AttentionFusion, etc.)
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ trainer.py         # Training loop and validation
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ common.py          # Utility functions
â”‚       â””â”€â”€ postprocess.py     # CTC decoding
â”œâ”€â”€ train.py                   # Main training script
â”œâ”€â”€ run_ablation.py            # Ablation study automation
â””â”€â”€ pyproject.toml             # Dependencies
```

---

## Technical Details

### Attention Fusion Module
Dynamically computes attention weights across temporal frames and fuses multi-frame features into a single representation before sequence modeling.

### Data Augmentation
- **Full mode**: Affine transforms, perspective warping, HSV adjustment, coarse dropout
- **Light mode**: Resize and normalize only
- **Scenario-B aware splitting**: Validation set prioritizes challenging scenarios to prevent overfitting